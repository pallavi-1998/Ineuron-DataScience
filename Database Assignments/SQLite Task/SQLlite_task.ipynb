{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task : dataset - https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\n",
    "\n",
    "q1 = try to find out a count of each and every word in a respective file return a list of tuple with word and its respective count\n",
    "sample example - [('sudh', 6 ) , ('kumar',3)]\n",
    "\n",
    "q2 = try to perform a reduce operation to get a count of all the word starting with same alphabet\n",
    "sample examle = [(a,56) , (b,34),...........]\n",
    "\n",
    "q3 = Try to filter out all the words from dataset .\n",
    ".001.abstract = abstract =.002 = delete\n",
    "\n",
    "q4 = create a tuple set of all the records avaialble in all the five file and then store it in sqllite DB .\n",
    "(aah,>=,354,fdsf,wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "## creating logging config\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"logfile.log\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s-%(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "# creating logging object\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-1 \n",
    "q1 = try to find out a count of each and every word in a respective file return a list of tuple with word and its respective countsample example - [('sudh', 6 ) , ('kumar',3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aaa', 1), ('aaas', 1), ('aactive', 1), ('aadvantage', 1), ('aaker', 1), ('aap', 1), ('aapg', 1), ('aaron', 1), ('aarp', 2), ('aas', 1), ('aau', 1), ('ab1890', 1), ('ab1x', 1), ('ab31x', 1), ('aba', 1), ('abacus', 1), ('abag', 1), ('abalone', 1), ('abandon', 2), ('abandoned', 2), ('abandoning', 2), ('abandonment', 1), ('abate', 1), ('abated', 1), ('abb', 2), ('abbott', 1), ('abbreviated', 1), ('abbreviation', 1), ('abby', 1), ('abc', 2)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def word_count(*files):\n",
    "    words = []\n",
    "    try:\n",
    "        for file in files:\n",
    "            with open(file, 'r', encoding='utf8') as f:\n",
    "                data = csv.reader(f)                \n",
    "                for i in data: \n",
    "                    words.append(i[0])\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in reading files \", e)\n",
    "        # print(e)\n",
    "    else:\n",
    "        unique_words = list(set(words))\n",
    "        unique_words.sort()\n",
    "        words_count = []\n",
    "        for i in unique_words:\n",
    "            words_count.append((i, words.count(i)))\n",
    "        logger.info(\"words count from  \"+ str(len(files)) + \" datasets are \"+ str(len(words_count)))   \n",
    "        return words_count            \n",
    "    \n",
    "task1 = word_count('vocab.enron.txt', 'vocab.kos.txt')\n",
    "print(task1[:30])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-2\n",
    "q2 = try to perform a reduce operation to get a count of all the word starting with same alphabet\n",
    "sample examle = [(a,56) , (b,34),...........]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3465), ('b', 3038), ('c', 5493), ('d', 3488), ('e', 2580), ('f', 2397), ('g', 1739), ('h', 1925), ('i', 2570), ('j', 546), ('k', 497), ('l', 1824), ('m', 3163), ('n', 1340), ('o', 1422), ('p', 4377), ('q', 273), ('r', 3537), ('s', 6372), ('t', 2752), ('u', 1506), ('v', 934), ('w', 1274), ('x', 48), ('y', 164), ('z', 58355)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def letter_count(*files):\n",
    "    letters = []\n",
    "    try:\n",
    "        for file in files:\n",
    "            with open(file, 'r', encoding='utf8') as f:\n",
    "                data = csv.reader(f)                \n",
    "                for i in data: \n",
    "                    if i[0][0] >= 'a' and i[0][0] <= 'z' or i[0][0] >= 'A' and i[0][0] <= 'Z':\n",
    "                        letters.append(i[0][0])\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in reading files \", e)\n",
    "    else:\n",
    "        unique_letters = list(set(letters))\n",
    "        unique_letters.sort()\n",
    "        letters_count = []\n",
    "        for i in unique_letters:\n",
    "            letters_count.append((i, letters.count(i)))\n",
    "        logger.info(\"letters count from  \"+ str(len(files))+\" datasets are \"+str(len(letters_count)))   \n",
    "        return letters_count            \n",
    "    \n",
    "task2 = letter_count('vocab.nips.txt', 'vocab.nytimes.txt')\n",
    "print(task2[:30])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-3\n",
    "q3 = Try to filter out all the words from dataset .\n",
    ".001.abstract = abstract =.002 = delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'aa', 'aaa', 'aaaa', 'aaad', 'aaamyloidosis', 'aaar', 'aaas', 'aab', 'aabb', 'aabr', 'aabstract', 'aac', 'aacc', 'aachen', 'aacn', 'aacocf', 'aact', 'aactivated', 'aactive', 'aad', 'aada', 'aadc', 'aadenosine', 'aado', 'aadrenergic', 'aadrenoceptor', 'aae', 'aaf', 'aafp']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def filter_words(*files):\n",
    "    words = []\n",
    "    try:\n",
    "        for file in files:\n",
    "            with open(file, 'r', encoding='utf8') as f:\n",
    "                data = csv.reader(f)              \n",
    "                for row_data in data:\n",
    "                    words.append(row_data[0])\n",
    "                \n",
    "                # return words\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in reading files \", e)\n",
    "        \n",
    "    else:\n",
    "        res = []\n",
    "        for word in words:\n",
    "            st = \"\"\n",
    "            for i in range(len(word)):\n",
    "                if word[i] >= 'a' and word[i] <= 'z' or word[i] >= 'A' and word[i] <= 'Z':\n",
    "                    st += word[i]\n",
    "            if st != \"\":\n",
    "                res.append(st)\n",
    "\n",
    "        filter_word = list(set(res))\n",
    "        filter_word.sort()\n",
    "        logger.info(\"filtered words from \"+str(len(files)) +\"datasets is/are \"+str(len(filter_word)) )       \n",
    "        return filter_word\n",
    "\n",
    "        \n",
    "task3 = filter_words('vocab.pubmed.txt')\n",
    "print(task3[:30])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-4\n",
    "q4 = create a tuple set of all the records avaialble in all the five file and then store it in sqllite DB .\n",
    "(aah,>=,354,fdsf,wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def set_of_all(*files):\n",
    "    all_words = []\n",
    "    try:\n",
    "        for file in files:\n",
    "            with open(file, 'r', encoding=\"utf8\") as f:\n",
    "                data = csv.reader(f)\n",
    "                words = []\n",
    "                for row_data in data:\n",
    "                    words.append(row_data[0])\n",
    "                all_words.append(words)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error('Error in reading files: ' + str(e))\n",
    "    else:\n",
    "        logger.info('Success Return of list of each files as combined list')\n",
    "                \n",
    "        return all_words\n",
    "\n",
    "task4 = set_of_all('vocab.enron.txt', 'vocab.kos.txt', 'vocab.nips.txt', 'vocab.nytimes.txt', 'vocab.pubmed.txt')\n",
    "\n",
    "t1 = task4[0]\n",
    "t2 = task4[1]\n",
    "t3 = task4[2]\n",
    "t4 = task4[3]\n",
    "t5 = task4[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aaa', 'aarp', 'a2i', 'aah', '>='),\n",
       " ('aaas', 'abandon', 'aaa', 'aahed', '>>'),\n",
       " ('aactive', 'abandoned', 'aaai', 'aaron', '>>>'),\n",
       " ('aadvantage', 'abandoning', 'aapo', 'aback', '>/='),\n",
       " ('aaker', 'abb', 'aat', 'abacus', '->'),\n",
       " ('aap', 'abc', 'aazhang', 'abajo', '--'),\n",
       " ('aapg', 'abcs', 'abandonment', 'abalone', '-->'),\n",
       " ('aaron', 'abdullah', 'abbott', 'abandon', '-/-'),\n",
       " ('aarp', 'ability', 'abbreviated', 'abandoned', '-/+'),\n",
       " ('aas', 'aboard', 'abcde', 'abandoning', '/-'),\n",
       " ('aau', 'abortion', 'abe', 'abandonment', '/+-'),\n",
       " ('ab1890', 'abortions', 'abeles', 'abandono', '..'),\n",
       " ('ab1x', 'abraham', 'abi', 'abarnard', '...'),\n",
       " ('ab31x', 'abrams', 'abilistic', 'abashed', '+-'),\n",
       " ('aba', 'abroad', 'abilities', 'abate', '+/'),\n",
       " ('abacus', 'absence', 'ability', 'abated', '+/--'),\n",
       " ('abag', 'absent', 'abl', 'abatement', '+/?'),\n",
       " ('abalone', 'absentee', 'able', 'abating', '+/+'),\n",
       " ('abandon', 'absolute', 'ables', 'abbey', '++'),\n",
       " ('abandoned', 'absolutely', 'ablex', 'abbot', '+++'),\n",
       " ('abandoning', 'abstain', 'ably', 'abbreviated', '=0'),\n",
       " ('abandonment', 'absurd', 'abnormal', 'abbreviation', '>0'),\n",
       " ('abate', 'abu', 'abort', 'abc', '-0'),\n",
       " ('abated', 'abuse', 'abound', 'abcnew', '+/0'),\n",
       " ('abb', 'abuses', 'abramowicz', 'abdicate', '0-'),\n",
       " ('abbott', 'academy', 'abrash', 'abdicated', '0%'),\n",
       " ('abbreviated', 'accent', 'abrupt', 'abdicating', '0+'),\n",
       " ('abbreviation', 'accept', 'abruptly', 'abdication', '.00'),\n",
       " ('abby', 'acceptable', 'abscissa', 'abdomen', '=.000'),\n",
       " ('abc', 'acceptance', 'absence', 'abdominal', '.000')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a list of all words in list of list\n",
    "zip_list = list(zip(t1,t2,t3,t4,t5))\n",
    "zip_list[:30]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQLite operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "try:\n",
    "    db = sqlite3.connect('vocab_db.db')\n",
    "    logger.info(\"Database Created: \"+ str(db))\n",
    "    cursor = db.cursor()\n",
    "    query = \"CREATE TABLE vocab_table(enron text, kos text, nips text, nytimes text, pubmed text)\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "    count = 0\n",
    "    for record in zip_list:\n",
    "        query = \"INSERT INTO vocab_table VALUES {}\".format(tuple(record))\n",
    "        cursor.execute(query)\n",
    "        count += 1\n",
    "    db.commit()\n",
    "\n",
    "    logger.info(\"Total records inserted : \"+ str(count))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(\"Something went wong \",e)\n",
    "\n",
    "finally:\n",
    "    db.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aaa', 'aarp', 'a2i', 'aah', '>='), ('aaas', 'abandon', 'aaa', 'aahed', '>>'), ('aactive', 'abandoned', 'aaai', 'aaron', '>>>'), ('aadvantage', 'abandoning', 'aapo', 'aback', '>/='), ('aaker', 'abb', 'aat', 'abacus', '->'), ('aap', 'abc', 'aazhang', 'abajo', '--'), ('aapg', 'abcs', 'abandonment', 'abalone', '-->'), ('aaron', 'abdullah', 'abbott', 'abandon', '-/-'), ('aarp', 'ability', 'abbreviated', 'abandoned', '-/+'), ('aas', 'aboard', 'abcde', 'abandoning', '/-'), ('aau', 'abortion', 'abe', 'abandonment', '/+-'), ('ab1890', 'abortions', 'abeles', 'abandono', '..'), ('ab1x', 'abraham', 'abi', 'abarnard', '...'), ('ab31x', 'abrams', 'abilistic', 'abashed', '+-'), ('aba', 'abroad', 'abilities', 'abate', '+/'), ('abacus', 'absence', 'ability', 'abated', '+/--'), ('abag', 'absent', 'abl', 'abatement', '+/?'), ('abalone', 'absentee', 'able', 'abating', '+/+'), ('abandon', 'absolute', 'ables', 'abbey', '++'), ('abandoned', 'absolutely', 'ablex', 'abbot', '+++'), ('abandoning', 'abstain', 'ably', 'abbreviated', '=0'), ('abandonment', 'absurd', 'abnormal', 'abbreviation', '>0'), ('abate', 'abu', 'abort', 'abc', '-0'), ('abated', 'abuse', 'abound', 'abcnew', '+/0'), ('abb', 'abuses', 'abramowicz', 'abdicate', '0-'), ('abbott', 'academy', 'abrash', 'abdicated', '0%'), ('abbreviated', 'accent', 'abrupt', 'abdicating', '0+'), ('abbreviation', 'accept', 'abruptly', 'abdication', '.00'), ('abby', 'acceptable', 'abscissa', 'abdomen', '=.000'), ('abc', 'acceptance', 'absence', 'abdominal', '.000')]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "db = sqlite3.connect('vocab_db.db')\n",
    "cursor = db.cursor()\n",
    "\n",
    "data = cursor.execute(\"SELECT * FROM vocab_table limit 30\")\n",
    "res = [i for i in data]\n",
    "print(res)\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8d101deb4f15aa80d1df610b470683654faf42c029e236d67b2441b6e2180d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
